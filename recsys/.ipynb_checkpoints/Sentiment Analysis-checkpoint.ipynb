{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb8f578b",
   "metadata": {},
   "source": [
    "# Organizacion del NoteBook\n",
    "Dado un review, el bot deicidirá si tiene un review positivo o negativo. Específicamente:\n",
    "\n",
    "Primero se extraen características para la regresión logística texto de entrenamiento\n",
    "Se implementa la regresión logística\n",
    "Se aplica regresión logística en una tarea de procesamiento de lenguaje natural\n",
    "Prueba usando tu regresión logística\n",
    "Realizar análisis de errores\n",
    "Se ejecuta un conjunto de datos de reseñas de peliculas.\n",
    "Ejecute la celda a continuación para cargar los paquetes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee91b56a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "import spacy\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d2863b",
   "metadata": {},
   "source": [
    "El archivo usado se encuentra en la ruta Recommender Systems/recsys/Data. En la siguiente celda se muestra un ejemplo de lo que contiene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "424bde20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 812 entries, 0 to 811\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   review  812 non-null    object\n",
      " 1   score   812 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 12.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df_news = pd.read_csv(\"Data/training_data.csv\")\n",
    "df_news.drop('Unnamed: 0', inplace=True, axis=1)\n",
    "\n",
    "df_news.head(2)\n",
    "df_news.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371bc76a",
   "metadata": {},
   "source": [
    "### Preprocesamiento de la Data:\n",
    "Para poder aprovechar mejor la data, primero se limpiarán los textos de tal forma que solo se evalue la información relevante que contienen.\n",
    "Por tal motivo, se buscan eliminar palabras que no son necesarias como por ejemplo aquellas que esten en otros idiomas, palabras innecesarias y signos de puntuación. Utilizaremos el modulo stopwords de NLTK para este proceso. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4d05489",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df1e47125438stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-c879e2be861d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'spanish'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0madditional_stopwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblack_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mstopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf1e47125438stop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madditional_stopwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'es_core_news_sm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df1e47125438stop' is not defined"
     ]
    }
   ],
   "source": [
    "black_list = ['más', 'mas', 'unir', 'paises', 'pais', 'espa', 'no', 'os', 'a', 'compa', 'acompa', 'off', 'and', 'grecia', 'the','it', 'to',\n",
    "              'd',  'et',  'dame',  'il',  'dans', 'that',  'as',   'for',  'it',  'elections',  'would',  'this',  'with', 'york', 'obama', 'chavez', 'gadafi']\n",
    "stop = set(stopwords.words('spanish'))\n",
    "additional_stopwords=set(black_list)\n",
    "stopwords = stop.union(additional_stopwords)\n",
    "nlp = spacy.load('es_core_news_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c045ca9",
   "metadata": {},
   "source": [
    "Adicionalmente creamos una serie de funciones para limpiar los textos de cada review, y solo quedarnos con las raices de las palabras en la mayoria de los casos, y eliminar ruido de palabras inncesarias.\n",
    "La funcio cleaner primero utiliza expresiones regulares, para eliminar tildes, urls y símbolos. Luego lematizaremos, es decir, convertimos ciertas palabras que funcionan mejor cuando estan juntas y solo dejamos adjetivos y sustantivos, luego stemmizamos, es decir dejamos la raiz de una palabra. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0847bd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner(word):\n",
    "  \"\"\"Build clean text.\n",
    "    Input:\n",
    "        word: a string of tweets\n",
    "\n",
    "    Output:\n",
    "        out_text: a list with lemmatize and stemmed and eliminated unnecesary words\n",
    "\n",
    "  \"\"\"\n",
    "  word = re.sub(r'((http|https)\\:\\/\\/)?[a-zA-Z0-9\\.\\/\\?\\:@\\-_=#]+\\.([a-zA-Z]){2,6}([a-zA-Z0-9\\.\\&\\/\\?\\:@\\-_=#])*', '', word, flags=re.MULTILINE)\n",
    "  word = re.sub(r'(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', \"\", word)\n",
    "  word = re.sub(r'ee.uu', 'eeuu', word)\n",
    "  word = re.sub(r'\\#\\.', '', word)\n",
    "  word = re.sub(r'\\n', '', word)\n",
    "  word = re.sub(r',', '', word)\n",
    "  word = re.sub(r'\\-', ' ', word)\n",
    "  word = re.sub(r'\\.{3}', ' ', word)\n",
    "  word = re.sub(r'a{2,}', 'a', word)\n",
    "  word = re.sub(r'é{2,}', 'é', word)\n",
    "  word = re.sub(r'i{2,}', 'i', word)\n",
    "  word = re.sub(r'ja{2,}', 'ja', word) \n",
    "  word = re.sub(r'á', 'a', word)\n",
    "  word = re.sub(r'é', 'e', word)\n",
    "  word = re.sub(r'í', 'i', word)\n",
    "  word = re.sub(r'ó', 'o', word)\n",
    "  word = re.sub(r'ú', 'u', word)  \n",
    "  word = re.sub('[^a-zA-Z]', ' ', word)\n",
    "  list_word_clean = []\n",
    "  for w1 in word.split(\" \"):\n",
    "    if  w1.lower() not in stopwords:\n",
    "      list_word_clean.append(w1.lower())\n",
    "\n",
    "  bigram_list = bigram[list_word_clean]  # use of bigram anda lemmatization to check if there are word that work better together\n",
    "  out_text = lemmatization(\" \".join(bigram_list))\n",
    "  stemmer = SnowballStemmer('spanish') # use of stemmer to eliminate suffix in words, NLTK recommends SnowBall but, it can be used other stemmers.\n",
    "  out_text = stemming(out_text, stemmer)\n",
    "  return out_text\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ']):\n",
    "    \"\"\"Lemmatize text.\n",
    "    Input:\n",
    "        texts: a list with the words of a text\n",
    "        allowed_postags: a list with Part of Speech used by spacy. Check out https://spacy.io/usage/linguistic-features for more options.\n",
    "    Output:\n",
    "        out_text: a list with lemmatize \n",
    "\n",
    "    \"\"\"\n",
    "    texts_out = [ token.text for token in nlp(texts) if token.pos_ in \n",
    "                 allowed_postags and token.text not in black_list and len(token.text)>2]\n",
    "    return texts_out\n",
    "\n",
    "def stemming(text_list, stemmer):\n",
    "    \"\"\"Lemmatize text.\n",
    "    Input:\n",
    "        texts: a list with the words of a text\n",
    "        stemmer: Stemmer used by NLTK module. Check out https://www.nltk.org/api/nltk.stem.html#module-nltk.stem for more options.\n",
    "    Output:\n",
    "        out_text: a list with stemmed text\n",
    "\n",
    "    \"\"\"\n",
    "    review_clean = []\n",
    "    for word in text_list:\n",
    "        stem_word = stemmer.stem(word)  # stemming word\n",
    "        review_clean.append(stem_word)\n",
    "\n",
    "    return review_clean\n",
    "\n",
    "def build_freqs(tweets, ys):\n",
    "    \"\"\"Build frequencies.\n",
    "    Input:\n",
    "        tweets: a list of tweets\n",
    "        ys: an m x 1 array with the sentiment label of each tweet\n",
    "            (either 0 or 1)\n",
    "    Output:\n",
    "        freqs: a dictionary mapping each (word, sentiment) pair to its\n",
    "        frequency\n",
    "    \"\"\"\n",
    "    # Convert np array to list since zip needs an iterable.\n",
    "    # The squeeze is necessary or the list ends up with one element.\n",
    "    # Also note that this is just a NOP if ys is already a list.\n",
    "    yslist = np.squeeze(ys).tolist()\n",
    "\n",
    "    # Start with an empty dictionary and populate it by looping over all tweets\n",
    "    # and over all processed words in each tweet.\n",
    "    freqs = {}\n",
    "    for y, tweet in zip(yslist, tweets):\n",
    "        \n",
    "        for word in tweet[0]:\n",
    "            pair = (word, y)\n",
    "            if pair in freqs:\n",
    "                freqs[pair] += 1\n",
    "            else:\n",
    "                freqs[pair] = 1\n",
    "\n",
    "    return freqs\n",
    "\n",
    "def normalize_bad_good_review(score):\n",
    "    \"\"\"Build numerical score.\n",
    "    Input:\n",
    "        score: a text showing if a movie is good or bad\n",
    "      \n",
    "    Output:\n",
    "        num_score: a numerical represetation for a score\n",
    "        \n",
    "    \"\"\"\n",
    "    if score == 'buena':\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e1f0a1",
   "metadata": {},
   "source": [
    "#### Ejemplo de una review luego de ser preprocesada:\n",
    "Aplicamos la funcion creada en todo el Dataframe y creamos una columna que contenga el resultado.\n",
    "Se hace lo mismo para el score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943b2f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = gensim.models.Phrases(df_news.review.to_list())\n",
    "df_news.review[0]\n",
    "cleaner(df_news.review[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b22409",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news['clean_review'] = df_news['review'].apply(cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5747dac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a77efce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news['normalize_score'] = df_news['score'].apply(normalize_bad_good_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bb4106",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news.head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da3d6ce",
   "metadata": {},
   "source": [
    "### Procesamiento de la Data y entrenamiento del algoritmo:\n",
    "Luego de limpiar la data procedemos a procesarla, es decir convertir el texto limpio en informacion que pueda ser evaluada, es decir copnvertir en numeros, y preferiblemente en vectores que representen la información. Existen varios métodos, espacio de vectores y producto interno, PCA, Word2Vec, Glove, para este ejercicio usaremos frecuencia de apariciones de palabras en textos. Los pasos a seguir serían:\n",
    "* Convertir listas en vectores usando numpy\n",
    "* Extracción de caracteristicas de reviews\n",
    "* Entrenamiento de algoritmo \n",
    "* Testeo de algoritmo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937b96b7",
   "metadata": {},
   "source": [
    "Conversion de los textos y scores en listas, adicional a eso partiremos el set que se ha recibido en dos: Uno para el entrenamiento del algoritmo y otro para el testeo y medir la precision. Para el split utilizamos sklearn cuyo modulo tiene una funcion de particion de datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69daee3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_array = np.asarray(df_news['clean_review'])\n",
    "y_array = np.asarray(df_news['normalize_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb398594",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ddbb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3234e93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_array, y_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa34f25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train[:,np.newaxis]\n",
    "x_test =  x_test[:,np.newaxis]\n",
    "y_train = y_train[:,np.newaxis]\n",
    "y_test = y_test[:,np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b739546f",
   "metadata": {},
   "source": [
    "Como resultado, obtenemos 4 vectores: 2 de entrenamiento y 2 de test. Con esta información, utilizamos los vectores de entrenamiento para extraer las caracteríticas de cada review, que es vbasicamente asignar la frecuencia de aparición a cada palabra en un review bueno o malo. enteoría, las palabras que más aparezcan en un texto bueno ayudarán a identificar si el review es bueno y lo mismo para una review mala."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98501d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd61e528",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0d4fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = build_freqs(x_train, y_train)\n",
    "freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8526815b",
   "metadata": {},
   "source": [
    " Luego de obtener las frecuencias asignadas para cada tupla de palabra y score, procederemos a extraer las características, que consiste en convertir todas las palabras en un vector representativo del texto. Para el ejercicio el vector resultante es de 1 x 3.\n",
    " Como algoritmo de clasifición se usa como kernel la regresión logística, cuya funcion viene dada por:\n",
    " $$ h(z) = \\frac{1}{1+\\exp^{-z}} \\tag{1}$$\n",
    "Donde $z$ corresponde a:\n",
    "\n",
    "$$z = \\theta_0 x_0 + \\theta_1 x_1 + \\theta_2 x_2$$\n",
    "con $[x_0,x_1,x_2]$ el vector representativo  del review y $[\\theta_0, \\theta_1, \\theta_2]$ los pesos optimos para identificar si una review es buena o mala. Para obtener estos pesos se utiliza el algoritmo de gradiente descendet en este ejercicio, que viene dado por:\n",
    "$$\\mathbf{\\theta} = \\mathbf{\\theta} - \\frac{\\alpha}{m} \\times \\left( \\mathbf{x}^T \\cdot \\left( \\mathbf{h-y} \\right) \\right)$$\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c0369a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(tweet, freqs):\n",
    "    '''\n",
    "    Input: \n",
    "        tweet: a list of words for one tweet\n",
    "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
    "    Output: \n",
    "        x: a feature vector of dimension (1,3)\n",
    "    '''\n",
    "    # process_tweet tokenizes, stems, and removes stopwords\n",
    "    word_l = tweet\n",
    "    \n",
    "    # 3 elements in the form of a 1 x 3 vector\n",
    "    x = np.zeros((1, 3)) \n",
    "    \n",
    "    #bias term is set to 1\n",
    "    x[0,0] = 1 \n",
    "    \n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    \n",
    "    # loop through each word in the list of words\n",
    "    for word in word_l:\n",
    "        \n",
    "        # increment the word count for the positive label 1\n",
    "        x[0,1] += freqs.get((word, 1.0),0)\n",
    "        \n",
    "        # increment the word count for the negative label 0\n",
    "        x[0,2] += freqs.get((word, 0.0),0)\n",
    "        \n",
    "    ### END CODE HERE ###\n",
    "    assert(x.shape == (1, 3))\n",
    "    return x\n",
    "\n",
    "def sigmoid(z): \n",
    "    '''\n",
    "    Input:\n",
    "        z: is the input (can be a scalar or an array)\n",
    "    Output:\n",
    "        h: the sigmoid of z\n",
    "    '''\n",
    "    \n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    # calculate the sigmoid of z\n",
    "    h = 1 / (1 + np.exp(-z))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return h\n",
    "\n",
    "# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "def gradientDescent(x, y, theta, alpha, num_iters):\n",
    "    '''\n",
    "    Input:\n",
    "        x: matrix of features which is (m,n+1)\n",
    "        y: corresponding labels of the input matrix x, dimensions (m,1)\n",
    "        theta: weight vector of dimension (n+1,1)\n",
    "        alpha: learning rate\n",
    "        num_iters: number of iterations you want to train your model for\n",
    "    Output:\n",
    "        J: the final cost\n",
    "        theta: your final weight vector\n",
    "    Hint: you might want to print the cost to make sure that it is going down.\n",
    "    '''\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    # get 'm', the number of rows in matrix x\n",
    "    m = x.shape[0]\n",
    "    \n",
    "    for i in range(0, num_iters):\n",
    "        \n",
    "        # get z, the dot product of x and theta\n",
    "        z = np.dot(x,theta)\n",
    "        \n",
    "        # get the sigmoid of z\n",
    "        h = sigmoid(z)\n",
    "        \n",
    "        # calculate the cost function\n",
    "        J = -1./m * (np.dot(y.transpose(), np.log(h)) + np.dot((1-y).transpose(),np.log(1-h)))    \n",
    "\n",
    "        # update the weights theta\n",
    "        theta = theta = theta - (alpha/m) * np.dot(x.transpose(),(h-y))\n",
    "        \n",
    "    ### END CODE HERE ###\n",
    "    J = float(J)\n",
    "    return J, theta\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "def test_logistic_regression(test_x, test_y, freqs, theta):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        test_x: a list of tweets\n",
    "        test_y: (m, 1) vector with the corresponding labels for the list of tweets\n",
    "        freqs: a dictionary with the frequency of each pair (or tuple)\n",
    "        theta: weight vector of dimension (3, 1)\n",
    "    Output: \n",
    "        accuracy: (# of tweets classified correctly) / (total # of tweets)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    \n",
    "    # the list for storing predictions\n",
    "    y_hat = []\n",
    "    \n",
    "    for tweet in test_x:\n",
    "        # get the label prediction for the tweet\n",
    "        y_pred = predict_tweet(tweet[0], freqs, theta)\n",
    "        \n",
    "        if y_pred > 0.5:\n",
    "            # append 1.0 to the list\n",
    "            y_hat.append(1)\n",
    "        else:\n",
    "            # append 0 to the list\n",
    "            y_hat.append(0)\n",
    "\n",
    "    # With the above implementation, y_hat is a list, but test_y is (m,1) array\n",
    "    # convert both to one-dimensional arrays in order to compare them using the '==' operator\n",
    "    accuracy = (y_hat==np.squeeze(test_y)).sum()/len(test_x)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335bd7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train[0,0])\n",
    "tmp1 = extract_features(x_train[0,0], freqs)\n",
    "print(tmp1)\n",
    "tmp2 = extract_features('blorb bleeeeb bloooob', freqs)\n",
    "print(tmp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf5a3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((len(x_train), 3))\n",
    "print(X.shape)\n",
    "for i in range(len(x_train)):\n",
    "    print(x_train[i,0])\n",
    "    X[i, :]= extract_features(x_train[i,0], freqs)\n",
    "    print(X[i, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799c4c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35c324d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce35b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317268db",
   "metadata": {},
   "source": [
    "Luego de aplicar la extraccion de caracteŕisticas, aplicamos el algoritmo de gradiente descendente en la funcíon logística."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857c4087",
   "metadata": {},
   "outputs": [],
   "source": [
    "J, theta = gradientDescent(X, Y, np.zeros((3, 1)), 1e-9, 100000)\n",
    "print(f\"The cost after training is {J:.8f}.\")\n",
    "print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(theta)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341bf357",
   "metadata": {},
   "source": [
    "### El vector de pesos resultante, se usa para predecir y testear la precision del algoritmo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcf6737",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_tweet(tweet, freqs, theta):\n",
    "    '''\n",
    "    Input: \n",
    "        tweet: a string\n",
    "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
    "        theta: (3,1) vector of weights\n",
    "    Output: \n",
    "        y_pred: the probability of a tweet being positive or negative\n",
    "    '''\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    \n",
    "    # extract the features of the tweet and store it into x\n",
    "    x = extract_features(tweet,freqs)\n",
    "    \n",
    "    # make the prediction using x and theta\n",
    "    y_pred = sigmoid(np.dot(x,theta))\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c0e1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print( '%s -> %f real: %f' % (x_test[26,0], predict_tweet(x_test[26,0], freqs, theta), y_test[26,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc602e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_logistic_regression(x_test, y_test, freqs, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7e14c2",
   "metadata": {},
   "source": [
    "Como resultado tenemos una precisión de 79.8% dadas por falsos positivos y positivos falsos, lo cual el algoritmo va a fallar en aproximadamente en 1 de 5 reviews que se le carguen. Hay que tener en cuenta que se usó un dataset pequeño,ademas de que es necesario mejorar el proceso de limpioza, pero con esto se muestra que es posible usar machine learning para determinar si una pelicula es buena o mala. Finalmente guaradeamos el valor de los pesos para usarlo en otras predicciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2addd706",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "outfile = open('Data/recsys','wb')\n",
    "info = {\"freqs\":freqs,\n",
    "       \"theta\":theta}\n",
    "pickle.dump(info,outfile)\n",
    "outfile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb4920b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
